{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96c1bb-8066-43fb-9e91-96f2ab5175e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import korean\n",
    "\n",
    "def koreanCleaner(text):\n",
    "    return \"\".join(korean.tokenize(text))\n",
    "\n",
    "def koreanFormatter(root_path, meta_file):\n",
    "    txt_file = os.path.join(root_path, meta_file)\n",
    "    items = []\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.split(\"|\")\n",
    "            wav_file = os.path.join(root_path, cols[0])\n",
    "            text = cols[2].strip()\n",
    "            text=koreanCleaner(text)\n",
    "            speaker_name = cols[3].strip()\n",
    "            items.append([text, wav_file, speaker_name])\n",
    "    return items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3a66-53fb-4a25-b420-349f7a58c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Trainer: Where the ‚ú®Ô∏è happens.\n",
    "# TrainingArgs: Defines the set of arguments of the Trainer.\n",
    "from TTS.trainer import Trainer, TrainingArgs\n",
    "\n",
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig, CharactersConfig, BaseAudioConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.glow_tts import GlowTTS\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "# we use the same path as this script as our training folder.\n",
    "output_path=\"./model/tts/glowtts/\"\n",
    "\n",
    "# DEFINE DATASET CONFIG\n",
    "# Set LJSpeech as our target dataset and define its path.\n",
    "# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\n",
    "dataset_config = [BaseDatasetConfig(name=\"kss\", meta_file_train=\"metadata.csv\", path=os.path.join(\"resample\",\"halfLife\"))]*50\n",
    "\n",
    "\n",
    "characters_config=CharactersConfig(\n",
    "    pad='_',\n",
    "    eos='~',\n",
    "    bos='^',\n",
    "    characters=\"\".join(list(korean._punctuation)+ korean._letters),\n",
    "    punctuations=korean._punctuation\n",
    ")\n",
    "\n",
    "\n",
    "# INITIALIZE THE TRAINING CONFIGURATION\n",
    "# Configure the model. Every config class inherits the BaseTTSConfig.\n",
    "config = GlowTTSConfig(\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"basic_cleaners\",\n",
    "    use_phonemes=False,\n",
    "    use_espeak_phonemes=False,\n",
    "    phoneme_language=None,\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    output_path=output_path,\n",
    "    datasets=dataset_config,\n",
    "    characters=characters_config,\n",
    ")\n",
    "\n",
    "\n",
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor(**config.audio.to_dict())\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# You can define your custom sample loader returning the list of samples.\n",
    "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
    "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True, formatter=koreanFormatter)\n",
    "eval_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d723bd-049b-48c4-a9e3-52246ab15959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = GlowTTS(config, speaker_manager=None)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the üê∏TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "trainer = Trainer(\n",
    "    TrainingArgs(restore_path=\"./model/tts/glowtts/coqui_tts-December-08-2021_03+15PM-0000000/checkpoint_190000.pth.tar\"),\n",
    "    config,\n",
    "    output_path,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    "    training_assets={\"audio_processor\": ap},  # assets are objetcs used by the models but not class members.\n",
    ")\n",
    "\n",
    "# AND... 3,2,1... üöÄ\n",
    "trainer.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93e946-04dd-41b7-8d7b-ef065ce5f714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
